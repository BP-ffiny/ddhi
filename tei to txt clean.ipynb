{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Read the TEI file\n",
    "with open(\"C:/Users/pothb/OneDrive/Dokumentumok/school 2024/ideal tavern stories/data/jstor tei/23040843_new.tei\", 'r', encoding='utf-8') as file:\n",
    "    tei_content = file.read()\n",
    "\n",
    "# Find all content between <s> and </s> tags\n",
    "sentences = re.findall(r'<s>(.*?)</s>', tei_content, re.DOTALL)\n",
    "\n",
    "# Print each extracted sentence\n",
    "for i, sentence in enumerate(sentences, 1):\n",
    "    print(f\"Sentence {i}: {sentence.strip()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Define the prefix of sentences to remove\n",
    "unwanted_prefix = \"This content downloaded\"\n",
    "\n",
    "# Read the TEI file\n",
    "with open(\"C:/Users/pothb/OneDrive/Dokumentumok/school 2024/ideal tavern stories/data/jstor tei/23040843_new.tei\", 'r', encoding='utf-8') as file:\n",
    "    tei_content = file.read()\n",
    "\n",
    "# Find all content between <s> and </s> tags\n",
    "sentences = re.findall(r'<s>(.*?)</s>', tei_content, re.DOTALL)\n",
    "\n",
    "# Filter out sentences that start with the unwanted prefix\n",
    "filtered_sentences = [sentence.strip() for sentence in sentences if not sentence.strip().startswith(unwanted_prefix)]\n",
    "\n",
    "# Print the filtered sentences\n",
    "for i, sentence in enumerate(filtered_sentences, 1):\n",
    "    print(f\"Filtered Sentence {i}: {sentence}\")\n",
    "\n",
    "# Save the filtered sentences to a file (optional)\n",
    "with open('filtered_sentences.txt', 'w', encoding='utf-8') as output_file:\n",
    "    for sentence in filtered_sentences:\n",
    "        output_file.write(sentence + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Define the keyword and the link pattern\n",
    "keyword = \"This content downloaded\"  # Replace with your keyword\n",
    "link_pattern = r'https?://[^\\s<]+'\n",
    "\n",
    "# Read the TEI file\n",
    "with open(\"C:/Users/pothb/OneDrive/Dokumentumok/school 2024/ideal tavern stories/data/jstor tei/23040843_new.tei\", 'r', encoding='utf-8') as file:\n",
    "    tei_content = file.read()\n",
    "\n",
    "# Find all content between <s> and </s> tags\n",
    "sentences = re.findall(r'<s>(.*?)</s>', tei_content, re.DOTALL)\n",
    "\n",
    "def remove_section(sentence):\n",
    "    \"\"\"Removes part of the sentence starting from the keyword to the next link.\"\"\"\n",
    "    # Search for the keyword and link in the sentence\n",
    "    keyword_index = sentence.find(keyword)\n",
    "    if keyword_index != -1:  # If the keyword is found\n",
    "        # Search for the next link after the keyword\n",
    "        link_match = re.search(link_pattern, sentence[keyword_index:])\n",
    "        if link_match:\n",
    "            link_end = keyword_index + link_match.end()\n",
    "            # Remove the section from the keyword to the link\n",
    "            return sentence[:keyword_index] + sentence[link_end:]\n",
    "    return sentence\n",
    "def remove_ref_sections(sentence):\n",
    "    \"\"\"\n",
    "    Removes all parts of the sentence that start with <ref and end with </ref>.\n",
    "    \"\"\"\n",
    "    # Pattern to match <ref ... </ref> sections\n",
    "    ref_pattern =  r'<ref\\s+type=\"bibr\".*?</ref>'\n",
    "    # Remove matched sections\n",
    "    return re.sub(ref_pattern, '', sentence)\n",
    "\n",
    "processed_sentences = [remove_section(remove_ref_sections(sentence.strip())) for sentence in sentences]\n",
    "\n",
    "\n",
    "# Print the processed sentence\n",
    "for i, sentence in enumerate(processed_sentences, 1):\n",
    "    print(f\"Processed Sentence {i}: {sentence}\")\n",
    "\n",
    "# Save the processed sentences to a file (optional)\n",
    "#with open('processed_sentences.txt', 'w', encoding='utf-8') as output_file:\n",
    " #   for sentence in processed_sentences:\n",
    "  #      output_file.write(sentence + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this is a put together version of above attempts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from glob import glob\n",
    "\n",
    "# Define the folder path containing the TEI files\n",
    "input_folder = \"C:/Users/pothb/OneDrive/Dokumentumok/school 2024/ideal tavern stories/data/jstor tei\"\n",
    "output_suffix = \"processed_\"  # Suffix for the new files\n",
    "\n",
    "# Define the keyword and the link pattern\n",
    "keyword = \"This content downloaded\"  # Replace with your keyword\n",
    "link_pattern = r'https?://[^\\s<]+'\n",
    "\n",
    "def remove_section(sentence):\n",
    "    \"\"\"Removes part of the sentence starting from the keyword to the next link.\"\"\"\n",
    "    # Search for the keyword and link in the sentence\n",
    "    keyword_index = sentence.find(keyword)\n",
    "    if keyword_index != -1:  # If the keyword is found\n",
    "        # Search for the next link after the keyword\n",
    "        link_match = re.search(link_pattern, sentence[keyword_index:])\n",
    "        if link_match:\n",
    "            link_end = keyword_index + link_match.end()\n",
    "            # Remove the section from the keyword to the link\n",
    "            return sentence[:keyword_index] + sentence[link_end:]\n",
    "    return sentence\n",
    "\n",
    "def remove_ref_sections(sentence):\n",
    "    \"\"\"\n",
    "    Removes all parts of the sentence that start with <ref and end with </ref>.\n",
    "    \"\"\"\n",
    "    # Pattern to match <ref ... </ref> sections\n",
    "    ref_pattern = r'<ref\\s+type=.*?</ref>'\n",
    "    # Remove matched sections\n",
    "    return re.sub(ref_pattern, '', sentence)\n",
    "\n",
    "def process_file(input_path, output_path):\n",
    "    \"\"\"\n",
    "    Process a single TEI file, removing unwanted sections, and save the result.\n",
    "    \"\"\"\n",
    "    with open(input_path, 'r', encoding='utf-8') as file:\n",
    "        tei_content = file.read()\n",
    "\n",
    "   \n",
    "        sentences = re.findall(r'<s>(.*?)</s>', tei_content, re.DOTALL)\n",
    "\n",
    "        # Process sentences to remove <ref> sections and unwanted parts\n",
    "        processed_sentences = [\n",
    "            remove_section(remove_ref_sections(sentence.strip())) for sentence in sentences\n",
    "        ]\n",
    "\n",
    "        # Join the sentences in the paragraph\n",
    "        processed_content = \"\\n\".join(processed_sentences)\n",
    "        \n",
    "   \n",
    "    \n",
    "    # Add an empty line between paragraphs\n",
    "    \n",
    "    with open(output_path, 'w', encoding='utf-8') as output_file:\n",
    "        output_file.write(processed_content)\n",
    "\n",
    "# Loop through all .tei files in the folder\n",
    "for file_path in glob(os.path.join(input_folder, \"*.tei\")):\n",
    "    file_name = os.path.basename(file_path)\n",
    "    name, ext = os.path.splitext(file_name)\n",
    "    output_path = os.path.join(input_folder, f\"{output_suffix}{name}{ext}\")\n",
    "\n",
    "    print(f\"Processing file: {file_name}\")\n",
    "    process_file(file_path, output_path)\n",
    "    print(f\"Processed file saved as: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "below is my attempt to include more paragraph strucutre, not to much acclaim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from glob import glob\n",
    "\n",
    "# Define the folder path containing the TEI files\n",
    "input_folder = \"C:/Users/pothb/OneDrive/Dokumentumok/school 2024/ideal tavern stories/data/jstor tei\"\n",
    "output_suffix = \"_processed\"  # Suffix for the new files\n",
    "\n",
    "# Define the keyword and the link pattern\n",
    "keyword = \"This content downloaded\"  # Replace with your keyword\n",
    "link_pattern = r'https?://[^\\s<]+'\n",
    "\n",
    "def remove_section(sentence):\n",
    "    \"\"\"Removes part of the sentence starting from the keyword to the next link.\"\"\"\n",
    "    keyword_index = sentence.find(keyword)\n",
    "    if keyword_index != -1:  # If the keyword is found\n",
    "        link_match = re.search(link_pattern, sentence[keyword_index:])\n",
    "        if link_match:\n",
    "            link_end = keyword_index + link_match.end()\n",
    "            # Remove the section from the keyword to the link\n",
    "            return sentence[:keyword_index] + sentence[link_end:]\n",
    "    return sentence\n",
    "\n",
    "def remove_ref_sections(sentence):\n",
    "    \"\"\"Removes all <ref> tags and their contents.\"\"\"\n",
    "    ref_pattern = r'<ref\\s+type=\"bibr\".*?</ref>'\n",
    "    return re.sub(ref_pattern, '', sentence)\n",
    "\n",
    "def process_file(input_path, output_path):\n",
    "    \"\"\"\n",
    "    Process a single TEI file, removing unwanted sections, and save the result.\n",
    "    \"\"\"\n",
    "    with open(input_path, 'r', encoding='utf-8') as file:\n",
    "        tei_content = file.read()\n",
    "\n",
    "    # Find all content within <p> tags\n",
    "    paragraphs = re.findall(r'<p>(.*?)</p>', tei_content, re.DOTALL)\n",
    "\n",
    "    processed_paragraphs = []\n",
    "\n",
    "    for paragraph in paragraphs:\n",
    "        # Find all sentences within <s> tags in the paragraph\n",
    "        sentences = re.findall(r'<s>(.*?)</s>', paragraph, re.DOTALL)\n",
    "        \n",
    "        # Process each sentence to remove unwanted parts\n",
    "        processed_sentences = [\n",
    "            remove_section(remove_ref_sections(sentence.strip())) for sentence in sentences\n",
    "        ]\n",
    "        \n",
    "        # Join sentences into a single paragraph\n",
    "        processed_paragraph = \" \".join(processed_sentences)\n",
    "        \n",
    "        # Append the cleaned paragraph\n",
    "        processed_paragraphs.append(processed_paragraph)\n",
    "\n",
    "    # Join paragraphs with an empty line between them\n",
    "    processed_content = \"\\n\\n\".join(processed_paragraphs)\n",
    "\n",
    "    # Write processed content to a new file\n",
    "    with open(output_path, 'w', encoding='utf-8') as output_file:\n",
    "        output_file.write(processed_content)\n",
    "\n",
    "# Loop through all .tei files in the folder\n",
    "for file_path in glob(os.path.join(input_folder, \"*.tei\")):\n",
    "    file_name = os.path.basename(file_path)\n",
    "    name, ext = os.path.splitext(file_name)\n",
    "    output_path = os.path.join(input_folder, f\"{name}{output_suffix}.txt\")\n",
    "\n",
    "    print(f\"Processing file: {file_name}\")\n",
    "    process_file(file_path, output_path)\n",
    "    print(f\"Processed file saved as: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "below is just a function to delete some files, not that important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "\n",
    "# Define the folder path where the files are located\n",
    "input_folder = \"C:/Users/pothb/OneDrive/Dokumentumok/school 2024/ideal tavern stories/data/jstor tei\"\n",
    "\n",
    "# Loop through all files in the folder that end with \"_processed\"\n",
    "for file_path in glob(os.path.join(input_folder, \"*_processed*\")):\n",
    "    try:\n",
    "        os.remove(file_path)  # Remove the file\n",
    "        print(f\"Deleted file: {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error deleting file {file_path}: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
